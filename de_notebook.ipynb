{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Udacity Data Engineering Capstone Project \n",
    "#### by KONE AZIZ R.\n",
    "\n",
    "#### Project Summary\n",
    "The purpose of the data engineering capstone project is to combine all the data engineering skills we have learned during the Udacity Data Engineering Journey.\n",
    "We will deal with nyc taxi trips records\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import configparser\n",
    "from sqlalchemy import create_engine\n",
    "import sql_queries as sq\n",
    "%reload_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "# Reading configuration file where Redshift connection informations are stored\n",
    "config.read_file(open('db.cfg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Getting our redshift datawarehouse configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "DWH_HOST               = config.get(\"CLUSTER\", \"HOST\")\n",
    "DWH_DB                 = config.get(\"CLUSTER\",\"DB_NAME\")\n",
    "DWH_DB_USER            = config.get(\"CLUSTER\",\"DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"CLUSTER\",\"DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"CLUSTER\",\"DB_PORT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Create a direct connection with our redshift database\n",
    "conn_string=f'postgresql://{DWH_DB_USER}:{DWH_DB_PASSWORD}@{DWH_HOST}:{DWH_PORT}/{DWH_DB}'\n",
    "db_engine = create_engine(conn_string)\n",
    "db_conn_url = f'jdbc:postgresql://{DWH_HOST}:{DWH_PORT}/{DWH_DB}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Start the spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types\n",
    "\n",
    "conf = SparkConf()  # create the configuration\n",
    "conf.set(\"spark.jars\", \"/home/workspace/postgresql-42.5.0.jar\") # This config helps spark to communicate with redshift and other postgreSQL database.\n",
    "conf=conf\n",
    "spark = SparkSession.builder \\\n",
    "        .config(conf=conf) \\\n",
    "        .appName(\"Udacity Data engineer capstone project\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# These has to be downloaded once\n",
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet\n",
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-01.parquet\n",
    "#!wget https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "mv taxi+_zone_lookup.csv personal_data/taxi_zone_lookup.csv\n",
    "mv yellow_tripdata_2022-01.parquet personal_data/\n",
    "mv green_tripdata_2022-01.parquet personal_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "payment_type_data = [\n",
    "        ('1','Credit card'),\n",
    "        ('2','Cash'),\n",
    "        ('3','No charge'),\n",
    "        ('4','Dispute'),\n",
    "        ('5','Unknow'),\n",
    "        ('6','Voided trip')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Load all data into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'personal_data'\n",
    "YEAR = 2022  #datetime.now().strftime('%Y')\n",
    "MONTH = '01' #datetime.now().strftime('%m')\n",
    "\n",
    "df_yellow = spark.read.parquet(f'{DATA_DIR}/yellow_tripdata_{YEAR}-{MONTH}.parquet')\n",
    "df_green = spark.read.parquet(f'{DATA_DIR}/green_tripdata_{YEAR}-{MONTH}.parquet')\n",
    "\n",
    "taxi_lookup_schema = types.StructType([\n",
    "    types.StructField(\"LocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"Borough\", types.StringType(), True),\n",
    "    types.StructField(\"Zone\", types.StringType(), True),\n",
    "    types.StructField(\"service_zone\", types.StringType(), True)\n",
    "])\n",
    "df_taxi_lookup = spark\\\n",
    "                        .read.option(\"header\",True) \\\n",
    "                        .csv(f'{DATA_DIR}/taxi_zone_lookup.csv' , schema=taxi_lookup_schema)\n",
    "\n",
    "\n",
    "df_pt = spark.createDataFrame(data=payment_type_data, schema = [\"TypeId\",\"TypeLabel\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_yellow.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: integer (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: double (nullable = true)\n",
      " |-- trip_type: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TypeId: string (nullable = true)\n",
      " |-- TypeLabel: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pt.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_taxi_lookup.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_createSqlStatement(df,tbname):\n",
    "    return pd.io.sql.get_schema(df.limit(5).toPandas(), name=tbname, con=db_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Data Cleaning - Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_yellow = df_yellow\\\n",
    ".withColumnRenamed('tpep_pickup_datetime' , 'pickup_datetime') \\\n",
    ".withColumnRenamed('tpep_dropoff_datetime' , 'dropoff_datetime')\\\n",
    ".withColumn('taxi_type' , F.lit('yellow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_green = df_green\\\n",
    ".withColumnRenamed('lpep_pickup_datetime' , 'pickup_datetime') \\\n",
    ".withColumnRenamed('lpep_dropoff_datetime' , 'dropoff_datetime') \\\n",
    ".withColumn('taxi_type' , F.lit('green'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We would like to join both dataframe green and yellow trips.\n",
    "1. We will only keep the columns they have in common \n",
    "2. We will then apply a union on these 2 dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "common_columns = []\n",
    "for column in df_yellow.columns:\n",
    "    if column in df_green.columns:\n",
    "        common_columns.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'RatecodeID',\n",
       " 'store_and_fwd_flag',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'congestion_surcharge',\n",
       " 'taxi_type']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_taxi_trips = df_green.select(common_columns).unionAll(df_yellow.select(common_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE fact_trips (\n",
      "\t\"VendorID\" BIGINT, \n",
      "\tpickup_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\tdropoff_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\tpassenger_count FLOAT(53), \n",
      "\ttrip_distance FLOAT(53), \n",
      "\t\"RatecodeID\" FLOAT(53), \n",
      "\tstore_and_fwd_flag TEXT, \n",
      "\t\"PULocationID\" BIGINT, \n",
      "\t\"DOLocationID\" BIGINT, \n",
      "\tpayment_type FLOAT(53), \n",
      "\tfare_amount FLOAT(53), \n",
      "\textra FLOAT(53), \n",
      "\tmta_tax FLOAT(53), \n",
      "\ttip_amount FLOAT(53), \n",
      "\ttolls_amount FLOAT(53), \n",
      "\timprovement_surcharge FLOAT(53), \n",
      "\ttotal_amount FLOAT(53), \n",
      "\tcongestion_surcharge FLOAT(53), \n",
      "\ttaxi_type TEXT\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "CREATE TABLE dim_location (\n",
      "\t\"LocationID\" INTEGER, \n",
      "\t\"Borough\" TEXT, \n",
      "\t\"Zone\" TEXT, \n",
      "\tservice_zone TEXT\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "CREATE TABLE dim_payment_type (\n",
      "\t\"TypeId\" TEXT, \n",
      "\t\"TypeLabel\" TEXT\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "CREATE TABLE dim_location (\n",
      "\t\"LocationID\" INTEGER, \n",
      "\t\"Borough\" TEXT, \n",
      "\t\"Zone\" TEXT, \n",
      "\tservice_zone TEXT\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_createSqlStatement(df_taxi_trips,'fact_trips'))\n",
    "print(get_createSqlStatement(df_taxi_lookup,'dim_location'))\n",
    "print(get_createSqlStatement(df_pt,'dim_payment_type'))\n",
    "print(get_createSqlStatement(df_taxi_lookup,'dim_location'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def loadIntoSpark(tbname):\n",
    "    \"\"\"\n",
    "        Load postgres table in spark.\n",
    "        - tbname: The table we want to load into spark \n",
    "    \"\"\"\n",
    "    df = spark.read\\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"url\",db_conn_url)\\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\")\\\n",
    "    .option(\"dbtable\", tbname) \\\n",
    "    .option(\"user\", f'{DWH_DB_USER}')\\\n",
    "    .option(\"password\", f'{DWH_DB_PASSWORD}')\\\n",
    "    .load()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def insertInto(df,table,partition=5,batchsize=10000,mode='append'):\n",
    "    \"\"\"\n",
    "        Insert Data into Redshit table\n",
    "        - df : The park dataframe\n",
    "        - table : The target table\n",
    "        - partition : Number of partitions default 5\n",
    "        - batchsize : Batch size default 1000\n",
    "        - mode : Mode of insertion. Can be overwrite / append / ignore / errorifexists or error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # For time saving purpose i limit tables with only 100 rows\n",
    "        df = df.limit(100)\n",
    "        print(f'There are {loadIntoSpark(table).count()} rows in {table} before insertion')\n",
    "        df.write\\\n",
    "            .format(\"jdbc\")\\\n",
    "            .option(\"url\", db_conn_url) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\")\\\n",
    "            .option(\"dbtable\", table) \\\n",
    "            .option(\"user\", f'{DWH_DB_USER}')\\\n",
    "            .option(\"password\", f'{DWH_DB_PASSWORD}')\\\n",
    "            .option(\"numPartitions\", partition)\\\n",
    "            .option(\"batchsize\", batchsize)\\\n",
    "            .mode(mode).save()\n",
    "        print(f'There are {loadIntoSpark(table).count()} rows in {table} after insertion')\n",
    "    except:\n",
    "        print('Insertion failed !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Insert Data into tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 rows in fact_trips before insertion\n",
      "There are 100 rows in fact_trips after insertion\n"
     ]
    }
   ],
   "source": [
    "insertInto(df_taxi_trips,'fact_trips',partition=5,batchsize=10000,mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 rows in dim_location before insertion\n",
      "There are 100 rows in dim_location after insertion\n"
     ]
    }
   ],
   "source": [
    "insertInto(df_taxi_lookup,'dim_location',partition=5,batchsize=10000,mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 rows in dim_payment_type before insertion\n",
      "There are 6 rows in dim_payment_type after insertion\n"
     ]
    }
   ],
   "source": [
    "insertInto(df_pt,'dim_payment_type',partition=5,batchsize=10000,mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "For time, we need to: </br>\n",
    "1- Load existing data into a dataframe  </br>\n",
    "2- Combine with the newly created dataframe with a join and exclude already existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a temporary table and extract time info from it\n",
    "df_taxi_trips = df_taxi_trips.limit(100)\n",
    "df_taxi_trips.registerTempTable('tmp_trips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Loading the existing time dimension data\n",
    "df_existing_time = loadIntoSpark('dim_time')\n",
    "df_existing_time.registerTempTable('tp_dim_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Creating the Time dimension table\n",
    "df_time = spark.sql(\"\"\"\n",
    "SELECT distinct\n",
    "       date(X.the_date)                                           AS date,\n",
    "       EXTRACT(year FROM X.the_date)                              AS year,\n",
    "       EXTRACT(quarter FROM X.the_date)                           AS quarter,\n",
    "       EXTRACT(month FROM X.the_date)                             AS month,\n",
    "       EXTRACT(day FROM X.the_date)                               AS day,\n",
    "       EXTRACT(week FROM X.the_date)                              AS week,\n",
    "       CASE WHEN weekday(X.the_date)  IN (6, 7) THEN true ELSE false END AS is_weekend\n",
    "FROM \n",
    "(SELECT pickup_datetime as the_date FROM tmp_trips\n",
    "UNION \n",
    "SELECT dropoff_datetime as the_date FROM tmp_trips) X\n",
    "Where X.the_date not in (select date from tp_dim_time)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE dim_time (\n",
      "\tdate DATE, \n",
      "\tyear INTEGER, \n",
      "\tquarter INTEGER, \n",
      "\tmonth INTEGER, \n",
      "\tday INTEGER, \n",
      "\tweek INTEGER, \n",
      "\tis_weekend BOOLEAN\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_createSqlStatement(df_time,'dim_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 rows in dim_time before insertion\n",
      "There are 3 rows in dim_time after insertion\n"
     ]
    }
   ],
   "source": [
    "insertInto(df_time,'dim_time',partition=5,batchsize=10000,mode='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Some tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: dwhuser@dwh'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "1- number of fact trips by day and taxi type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwh.c3ujdkixlmmf.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "0 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>date</th>\n",
       "        <th>taxi_type</th>\n",
       "        <th>nb_trips</th>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT \n",
    "    t.date, \n",
    "    p.taxi_type , \n",
    "    count(p.*) nb_trips \n",
    "from fact_trips p \n",
    "inner join dim_time t on p.pickup_datetime = t.date\n",
    "group by  \n",
    "t.date, \n",
    "p.taxi_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "2- number of fact trips by day and pick up location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwh.c3ujdkixlmmf.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "0 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>date</th>\n",
       "        <th>zone</th>\n",
       "        <th>nb_trips</th>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT \n",
    "    t.date, \n",
    "    l.zone,\n",
    "    count(p.*) nb_trips \n",
    "from fact_trips p \n",
    "inner join dim_time t on p.pickup_datetime = t.date\n",
    "inner join dim_location l on l.LocationID = p.PULocationID\n",
    "group by t.date , l.zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "3 - Average, Max and Min fare amount by taxi type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwh.c3ujdkixlmmf.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>taxi_type</th>\n",
       "        <th>avg_amount</th>\n",
       "        <th>max_amount</th>\n",
       "        <th>min_amount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>green</td>\n",
       "        <td>13.155</td>\n",
       "        <td>61.5</td>\n",
       "        <td>2.5</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('green', 13.155, 61.5, 2.5)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT \n",
    "    p.taxi_type , \n",
    "    AVG(p.fare_amount) avg_amount ,\n",
    "    MAX(p.fare_amount) max_amount ,\n",
    "    MIN(p.fare_amount) min_amount \n",
    "from fact_trips p \n",
    "group by p.taxi_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Result show in Tests above are not relevant as I set a limit on data before loading them.**\n",
    "**One can remove .limit(100) and run again all tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
